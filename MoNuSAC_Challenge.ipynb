{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2225a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3245\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_00.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_00.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_01.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_01.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_02.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_02.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_03.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_03.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_04.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_04.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_10.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_10.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_11.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_11.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_12.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_12.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_13.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_13.npy\n",
      "TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_14.tif | TCGA-MP-A4T7-01Z-00-DX1/TCGA-MP-A4T7-01Z-00-DX1_3_mask_14.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.keras.preprocessing.image as img_preproc\n",
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from PIL import ImageOps\n",
    "from models import get_model\n",
    "import json\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dir = \"/data/datasets/MoNuSAC_images_and_annotations/\"\n",
    "target_dir = \"/data/datasets/MoNuSAC_bin_masks/\"\n",
    "img_size = (160, 160)\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "\n",
    "with open('/data/datasets/processed_dataset.json', 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "input_img_paths = dataset['images']    \n",
    "target_img_paths = dataset['masks']\n",
    "    \n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)\n",
    "    \n",
    "assert len(input_img_paths)>0,\"input_dir is empty\"\n",
    "assert len(input_img_paths)>0,\"target_dir is empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a796b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d15cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 00:41:16.457627: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-27 00:41:16.477946: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1896520000 Hz\n",
      "2022-12-27 00:41:16.478575: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3210250 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-27 00:41:16.478585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 80, 80, 32)   896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 80, 80, 32)   128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 80, 80, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 80, 80, 32)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 80, 80, 64)   2400        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 80, 80, 64)   256         separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 80, 80, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 80, 80, 64)   4736        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 80, 80, 64)   256         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 40, 40, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 40, 40, 64)   2112        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 40, 40, 64)   0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 40, 40, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 40, 40, 128)  8896        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 40, 40, 128)  512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 40, 40, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 40, 40, 128)  17664       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 40, 40, 128)  512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 20, 20, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 20, 20, 128)  8320        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 20, 20, 128)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 20, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 20, 20, 256)  34176       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 20, 256)  1024        separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 20, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 20, 20, 256)  68096       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 20, 256)  1024        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 10, 256)  33024       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 10, 256)  0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 10, 10, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 10, 10, 256)  590080      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 10, 10, 256)  1024        conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 10, 10, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 10, 10, 256)  590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 10, 10, 256)  1024        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 20, 20, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 20, 20, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 20, 256)  65792       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 20, 20, 256)  0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 20, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 20, 20, 128)  295040      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 20, 128)  512         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 20, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 20, 20, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 20, 128)  512         conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 40, 40, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 40, 40, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 40, 40, 128)  32896       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 40, 40, 128)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 40, 40, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 40, 40, 64)   73792       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 40, 40, 64)   256         conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 40, 40, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 40, 40, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 40, 40, 64)   256         conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 80, 80, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 80, 80, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 80, 80, 64)   8256        up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 80, 80, 64)   0           up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 80, 80, 64)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 80, 80, 32)   18464       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 80, 80, 32)   128         conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 80, 80, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 80, 80, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 80, 80, 32)   128         conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 160, 160, 64) 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 160, 160, 32) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 160, 160, 32) 2080        up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 160, 160, 32) 0           up_sampling2d_6[0][0]            \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 160, 160, 5)  1445        add_6[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,059,557\n",
      "Trainable params: 2,055,781\n",
      "Non-trainable params: 3,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "K.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10677718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataset import Challenge\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "val_samples = 50\n",
    "#random.Random(1337).shuffle(input_img_paths)\n",
    "#random.Random(1337).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = Challenge(\n",
    "    batch_size, img_size, input_dir, target_dir, train_input_img_paths, train_target_img_paths, shuffle=False, augment=False\n",
    ")\n",
    "val_gen = Challenge(\n",
    "    batch_size, img_size, input_dir, target_dir, val_input_img_paths, val_target_img_paths, shuffle=False, augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da270a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input_img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11468186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_target_img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6435fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3f2fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "399/399 [==============================] - 187s 468ms/step - loss: 2.9417 - accuracy: 0.9964 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "399/399 [==============================] - 203s 510ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "399/399 [==============================] - 213s 534ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "399/399 [==============================] - 210s 526ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "399/399 [==============================] - 210s 525ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "399/399 [==============================] - 208s 521ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "399/399 [==============================] - 208s 521ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "399/399 [==============================] - 207s 519ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "399/399 [==============================] - 209s 523ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "399/399 [==============================] - 208s 522ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "399/399 [==============================] - ETA: 0s - loss: 2.9281 - accuracy: 1.0000Restoring model weights from the end of the best epoch.\n",
      "399/399 [==============================] - 208s 522ms/step - loss: 2.9281 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\"monusac-challenge_segmentation_baseline.h5\", save_best_only=True),\n",
    "    EarlyStopping(min_delta=0.01, patience=10, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the model for 50 epochs, in reality it will stop earlier because of the EarlyStopping\n",
    "epochs = 50\n",
    "history = model.fit(train_gen, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=val_gen, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7775931e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Baseline model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b8112",
   "metadata": {},
   "source": [
    "# Model evaluation - validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dbf3a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ISICChallenge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193497/2528045955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msegmentation_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m val_gen = ISICChallenge(1, \n\u001b[0m\u001b[1;32m      4\u001b[0m                         \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mval_input_img_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ISICChallenge' is not defined"
     ]
    }
   ],
   "source": [
    "from segmentation_metrics import calculate_metrics\n",
    "\n",
    "val_gen = ISICChallenge(1, \n",
    "                        img_size, \n",
    "                        val_input_img_paths, \n",
    "                        val_target_img_paths, \n",
    "                        shuffle=False, \n",
    "                        augment=False)\n",
    "\n",
    "val_preds = model.predict(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((len(val_input_img_paths),) + img_size + (1,), dtype=\"uint8\")\n",
    "for i, mask_path in enumerate(val_target_img_paths):\n",
    "    mask = np.array(load_img(mask_path, target_size=img_size, color_mode=\"grayscale\"))\n",
    "    mask[mask>0] = 1\n",
    "    mask = np.expand_dims(mask, 2)\n",
    "    y[i] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.squeeze(y)\n",
    "pr_masks = np.argmax(val_preds, axis=-1)\n",
    "y.shape, pr_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y.shape==pr_masks.shape, \"number of test samples doesn't match predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(y, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0ac3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_img = 5\n",
    "pr_masks = np.argmax(val_preds, axis=-1)\n",
    "    \n",
    "for img_path, mask_path, pr_mask in zip(val_input_img_paths[:max_img], \n",
    "                                        val_target_img_paths[:max_img], \n",
    "                                        pr_masks[:max_img]):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    img = load_img(img_path, target_size=img_size)\n",
    "    mask = load_img(mask_path, target_size=img_size, color_mode=\"grayscale\")\n",
    "    prediction = np.expand_dims(pr_mask, axis=-1)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(np.array(img))  # convert CHW -> HWC\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(np.array(mask)) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Ground truth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(prediction) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a56af",
   "metadata": {},
   "source": [
    "# Model evaluation - Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"images/test/\"\n",
    "target_dir = \"annotations/test/\"\n",
    "\n",
    "test_input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "test_target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(test_input_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = ISICChallenge(\n",
    "    1, img_size, test_input_img_paths, test_target_img_paths, shuffle=False, augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf975a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5013fa",
   "metadata": {},
   "source": [
    "### load all masks as used in training to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81db082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((len(test_input_img_paths),) + img_size + (1,), dtype=\"uint8\")\n",
    "for i, mask_path in enumerate(test_target_img_paths):\n",
    "    mask = np.array(load_img(mask_path, target_size=img_size, color_mode=\"grayscale\"))\n",
    "    mask[mask>0] = 1\n",
    "    mask = np.expand_dims(mask, 2)\n",
    "    y[i] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42714403",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.squeeze(y)\n",
    "pr_masks = np.argmax(test_preds, axis=-1)\n",
    "y.shape, pr_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y.shape==pr_masks.shape, \"number of test samples doesn't match predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2431bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(y, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f3171",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57053f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_img = 3\n",
    "    \n",
    "for img_path, mask_path, pr_mask in zip(test_input_img_paths[:max_img], \n",
    "                                        test_target_img_paths[:max_img], \n",
    "                                        pr_masks[:max_img]):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    img = load_img(img_path, target_size=img_size)\n",
    "    mask = load_img(mask_path, target_size=img_size, color_mode=\"grayscale\")\n",
    "    prediction = np.expand_dims(pr_mask, axis=-1)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(np.array(img))  # convert CHW -> HWC\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(np.array(mask)) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Ground truth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(prediction) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeaafde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b3ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
