{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f57ae94",
   "metadata": {},
   "source": [
    "# MoNuSAC Challenge\n",
    "Student: Braian Olmiro Dias\n",
    "\n",
    "## Set-up of the project\n",
    "Based on the data available on the [MoNuSAC Challenge 2020](https://monusac-2020.grand-challenge.org/Data/), a model was trained for **detecting four type of cells in H&E stained tissue images**. \n",
    "\n",
    "The libraries used were:\n",
    "\n",
    "* Tensorflow 2.3.0\n",
    "* Scikit-learn (for metrics)\n",
    "* Matplotlib (for plotting)\n",
    "* Numpy\n",
    "\n",
    "Originally, only 209 images were available for training. However, since the data has a variety of image sizes, it is possible to crop them in smaller images and thus increasing the number of images.\n",
    "\n",
    "In the next sections It will be briefly covered the details about data processing and model training.\n",
    "\n",
    "## Data\n",
    "First, the data available for the challenge is very diverse, with H&E stained tissue images from 4 different organs (prostate, breats, kidney, lungs). The dataset contains 209 images from 46 different patients and the distribution of the 4 classes (Epithelial, Lymphocyte, Neutrophil, Macrophage) is unbalanced. The number of Lymphocytes, for example, is less than 2% of the total cells in the dataset. Also, due to the data being collected in different hospitals and for different organs, the size of the image varies a lot. Below is a histogram showing the distribution of image width to demonstrate the problem.\n",
    "\n",
    "![fig 1](images/image_hist.png)\n",
    "\n",
    "Due to this variation in size, my first experiments with only image resize (to 160x160) didn't show good results, since some images will shrink by almost 10 times, making the cells too small.\n",
    "To overcome this issue, I decided to split the images into `patches` of size 160x160. For example, an image with original size 1600x1600 will generate 100 patches of size 160x160. For sizes that are not multiple of 160, I discard the last patches horizontally and vertically. The strategy is demostrated below, where the red area is multiple of 160 and will be divided into 4 patches.\n",
    "\n",
    "![fig 2](images/original_image_2.png)\n",
    "\n",
    "![fig 3](images/patches.png)\n",
    "\n",
    "After the division of images into patches, the final dataset has 3225 images.\n",
    "For training, validation and test set, I opted to randomly split the images, using 80% for training, 10% for validation and 10% for testing.\n",
    "\n",
    "Unfortunately due to the time spent on reading and transforming the data, I didn't have time to test any data augmentation pipeline. However, basic augmentation such as rotating the image should improve the results.\n",
    "\n",
    "## Model\n",
    "The model used in the project was adapted from the Practice block 1, a **U-Net Xception-style** architecture. I did not change the architecture, the only notable change was the number of classes that now is 5(background and the 4 different cell types).\n",
    "\n",
    "I tested different optimization functions: Adam, RMSProp and SGD with Momentum. The model was trained with `sparse_categorical_crossentropy` loss. `Adam` optimizer proven to be the best one since RMSProp was not converging.\n",
    "\n",
    "\n",
    "## Results\n",
    "The model with Adam optimizer and the image patching converged quikly and showed the best results. Since all the experiments were conducted using only CPU, the training time was around 40 minutes to train for 10 epochs. The best model training performed as the image below.\n",
    "\n",
    "![fig 4](images/training_curves.png)\n",
    "\n",
    "In terms of qualitative results, the segmentation mask produced was very accurate, as shown below in a sample from the test set.\n",
    "\n",
    "![fig 5](images/sample.png)\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "The MoNuSAC challenge dataset was complicated to deal with, since the data is unbalanced and with different sizes. Most of the work done in this assignment was related to reading the original files and transforming to a suitable format for training using Keras and Tensorflow. Due to the heterogeneity of the data, different sizes had to be resized or cropped, and this operations are often time consuming for an ordinary laptop.\n",
    "For the complete code and report please refer to `data_preprocessing` and `MoNuSAC_Challenge` jupyter notebooks. The whole source code can be seen here https://github.com/bodias/MoNuSAC_Challenge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c4f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
